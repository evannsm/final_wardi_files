{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "\n",
    "sim = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50k_iris_sim_expanded_more4.npy because sim is True\n"
     ]
    }
   ],
   "source": [
    "data_all = np.load('50k_iris_sim_expanded_more4.npy' if sim else '50k_holybro_expanded_more2.npy' )\n",
    "print(f\"Loaded {'50k_iris_sim_expanded_more4.npy' if sim else '50k_holybro_expanded_more2.npy'} because sim is {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader.dataset.data.shape = (40000, 17)\n",
      "test_dataloader.dataset.data.shape = (10000, 17)\n"
     ]
    }
   ],
   "source": [
    "class QuadrotorDataset(Dataset):\n",
    "  def __init__(self, state_size = 9, ctrlinput_size = 4, training=True, data_all = data_all):\n",
    "    self.input_size = state_size + ctrlinput_size\n",
    "    self.output_size = ctrlinput_size\n",
    "    np.random.shuffle(data_all)\n",
    "    train_size = int(len(data_all)*0.8)\n",
    "    if training:\n",
    "      self.data = data_all[0:train_size]\n",
    "    else:\n",
    "      self.data = data_all[train_size:]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    inputs = self.data[idx, 0: self.input_size]\n",
    "    outputs = self.data[idx, self.input_size:]\n",
    "    return torch.FloatTensor(inputs), torch.FloatTensor(outputs).view(4)\n",
    "  \n",
    "train_dataset = QuadrotorDataset(training=True)\n",
    "test_dataset = QuadrotorDataset(training=False)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"{train_dataloader.dataset.data.shape = }\")\n",
    "print(f\"{test_dataloader.dataset.data.shape = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax import linen as nn\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import random\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "fake_train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "for data in fake_train_dataloader:\n",
    "    input_data, output_data = data\n",
    "    print(input_data.shape)\n",
    "    print(output_data.shape)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = [tensor([[ 0.2288,  0.2288,  1.1144,  0.1144,  0.1144,  0.1144,  0.7189,  0.7189,\n",
      "          0.7189, 15.0446,  0.1030,  0.1030,  0.1030]]), tensor([[-1.9420,  0.7672,  2.4488,  0.8013]])]\n",
      "torch.Size([1, 13])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "for data in fake_train_dataloader:\n",
    "    print(f\"{data = }\")\n",
    "    input_data, output_data = data\n",
    "    print(input_data.shape)\n",
    "    print(output_data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\n",
      " [ 1.2459718  -0.860127    0.13101679  2.3912866 ]\n",
      "true:\n",
      " [-1.94202399  0.76719505  2.44880295  0.8012557 ]\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, feat in enumerate(self.features):\n",
    "    #   print(f\"{i = }, {feat = }\")\n",
    "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "model = FeedForward(features=[13, 128, 256, 256, 128, 4])\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = fake_train_dataloader.dataset.data[0][0:13]\n",
    "params = model.init(key1, x)\n",
    "y = model.apply(params, x)\n",
    "print('pred:\\n', y[0:5])\n",
    "print('true:\\n', train_dataloader.dataset.data[0][13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flax Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003  # Gradient step size.\n",
    "\n",
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    pred = model.apply(params, x_batched)\n",
    "    SE = (pred - y_batched)**2\n",
    "    return jnp.mean(SE)\n",
    "    \n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  3.3695362e-09\n",
      "Loss step 10:  4.6185278e-14\n",
      "Loss step 20:  6.899459e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "  for data in train_dataloader:\n",
    "      x_samples, y_samples = data\n",
    "      x_samples = jnp.array(x_samples.numpy())  # Convert to jax.numpy array\n",
    "      y_samples = jnp.array(y_samples.numpy())  # Convert to jax.numpy array\n",
    "      loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "      updates, opt_state = tx.update(grads, opt_state)\n",
    "      params = optax.apply_updates(params, updates)\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:\n",
      " [-1.9420134   0.7671896   2.4487896   0.80125034]\n",
      "true:\n",
      " [-1.94202399  0.76719505  2.44880295  0.8012557 ]\n"
     ]
    }
   ],
   "source": [
    "x = fake_train_dataloader.dataset.data[0][0:13]\n",
    "y = model.apply(params, x)\n",
    "print('pred:\\n', y[0:5])\n",
    "print('true:\\n', train_dataloader.dataset.data[0][13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.982415e-11\n",
      "Mean Test Loss:  8.902072e-11\n"
     ]
    }
   ],
   "source": [
    "test_array = []\n",
    "for data in test_dataloader:\n",
    "    x_samples, y_samples = data\n",
    "    x_samples = jnp.array(x_samples.numpy())  # Convert to jax.numpy array\n",
    "    y_samples = jnp.array(y_samples.numpy())  # Convert to jax.numpy array\n",
    "    loss_val, _ = loss_grad_fn(params, x_samples, y_samples)\n",
    "    print('Test Loss: ', loss_val)\n",
    "    test_array.append(loss_val)\n",
    "print('Mean Test Loss: ', np.mean(test_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import serialization\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "bytes_output = serialization.to_bytes(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.23633464,  0.08877068, -0.56622684, -0.44590765,  0.41151434,\n",
       "        0.37425488, -0.37730283,  0.07838764,  0.17399848, -0.3088262 ,\n",
       "        0.18105392,  0.4158016 ,  0.13710581,  0.2363445 , -0.5415333 ],      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['layers_1']['kernel'][0,:][15:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.23633464,  0.08877068, -0.56622684, -0.44590765,  0.41151434,\n",
       "        0.37425488, -0.37730283,  0.07838764,  0.17399848, -0.3088262 ,\n",
       "        0.18105392,  0.4158016 ,  0.13710581,  0.2363445 , -0.5415333 ],      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_output['params']['layers_1']['kernel'][0,:][15:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sim_params_expanded3.bin\", \"wb\") as f:\n",
    "    f.write(bytes_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLoad(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, feat in enumerate(self.features):\n",
    "    #   print(f\"{i = }, {feat = }\")\n",
    "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "modelnew = FeedForwardLoad(features=[13, 128, 256, 256, 128, 4])\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = fake_train_dataloader.dataset.data[0][0:13]\n",
    "init_params = modelnew.init(key2, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94202399,  0.76719505,  2.44880295,  0.8012557 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_train_dataloader.dataset.data[0][13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.01019191, -0.00296605, -0.08019079,  0.05023021,  0.04959011,\n",
       "       -0.02081057, -0.13012654, -0.05989638, -0.04309102,  0.0091062 ,\n",
       "       -0.0193517 , -0.09684327,  0.04341459,  0.00183117, -0.02519199],      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_params['params']['layers_3']['kernel'][0,:][15:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "with open(\"sim_params_expanded3.bin\", \"rb\") as f:\n",
    "    loaded_bytes = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore parameters from bytes\n",
    "loaded_params = serialization.from_bytes(init_params, loaded_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23633464,  0.08877068, -0.56622684, -0.44590765,  0.41151434,\n",
       "        0.37425488, -0.37730283,  0.07838764,  0.17399848, -0.3088262 ,\n",
       "        0.18105392,  0.4158016 ,  0.13710581,  0.2363445 , -0.5415333 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_params['params']['layers_1']['kernel'][0,:][15:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    pred = modelnew.apply(params, x_batched)\n",
    "    SE = (pred - y_batched)**2\n",
    "    return jnp.mean(SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.901041e-11\n",
      "Test Loss:  8.982415e-11\n",
      "Mean Test Loss:  8.902072e-11\n"
     ]
    }
   ],
   "source": [
    "test_array = []\n",
    "for data in test_dataloader:\n",
    "    x_samples, y_samples = data\n",
    "    x_samples = jnp.array(x_samples.numpy())  # Convert to jax.numpy array\n",
    "    y_samples = jnp.array(y_samples.numpy())  # Convert to jax.numpy array\n",
    "    loss_val  = mse(loaded_params, x_samples, y_samples)\n",
    "    print('Test Loss: ', loss_val)\n",
    "    test_array.append(loss_val)\n",
    "print('Mean Test Loss: ', np.mean(test_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def model_apply(params, x):\n",
    "    return modelnew.apply(params, x)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log = []\n",
    "for i in range(100):\n",
    "    t0 = time.time()\n",
    "    pred = model_apply(params, x)\n",
    "    time_log.append(time.time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0019253134727478028)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(time_log).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputNN:  [-0.4263151   0.1881113   0.5163594   0.12628472]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def model_apply(params, x):\n",
    "    return modelnew.apply(params, x)\n",
    "\n",
    "from functools import partial\n",
    "apply_model = partial(model_apply, params)\n",
    "\n",
    "\n",
    "import time\n",
    "position_now = np.array([0.2342, .682, -1.2342,   0.0, 0.0, 0.0,   0.0, 0.0, 0.0])\n",
    "input_data = np.array([0.324, 0.8953, 0.131, 0.09213])\n",
    "\n",
    "dataNN = np.concatenate((position_now, input_data))\n",
    "\n",
    "t0 = time.time()\n",
    "outputNN = apply_model(dataNN)\n",
    "print('outputNN: ', outputNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018281936645507812\n",
      "0.0004107952117919922\n",
      "8.916854858398438e-05\n",
      "0.0017545223236083984\n",
      "0.00025010108947753906\n",
      "0.00013685226440429688\n",
      "6.413459777832031e-05\n",
      "4.410743713378906e-05\n",
      "4.673004150390625e-05\n",
      "4.8160552978515625e-05\n",
      "6.222724914550781e-05\n",
      "5.1021575927734375e-05\n",
      "5.745887756347656e-05\n",
      "5.435943603515625e-05\n",
      "5.698204040527344e-05\n",
      "5.2928924560546875e-05\n",
      "5.3882598876953125e-05\n",
      "4.744529724121094e-05\n",
      "3.552436828613281e-05\n",
      "3.218650817871094e-05\n",
      "3.24249267578125e-05\n",
      "3.1948089599609375e-05\n",
      "3.6716461181640625e-05\n",
      "3.886222839355469e-05\n",
      "3.457069396972656e-05\n",
      "5.0067901611328125e-05\n",
      "4.1484832763671875e-05\n",
      "4.649162292480469e-05\n",
      "4.172325134277344e-05\n",
      "4.220008850097656e-05\n",
      "3.600120544433594e-05\n",
      "3.62396240234375e-05\n",
      "4.887580871582031e-05\n",
      "4.458427429199219e-05\n",
      "4.124641418457031e-05\n",
      "4.076957702636719e-05\n",
      "4.5299530029296875e-05\n",
      "3.8623809814453125e-05\n",
      "4.00543212890625e-05\n",
      "4.00543212890625e-05\n",
      "3.790855407714844e-05\n",
      "3.528594970703125e-05\n",
      "3.5762786865234375e-05\n",
      "3.600120544433594e-05\n",
      "3.5762786865234375e-05\n",
      "3.314018249511719e-05\n",
      "3.3855438232421875e-05\n",
      "3.0994415283203125e-05\n",
      "3.0040740966796875e-05\n",
      "3.4809112548828125e-05\n",
      "3.695487976074219e-05\n",
      "4.863739013671875e-05\n",
      "3.981590270996094e-05\n",
      "4.8160552978515625e-05\n",
      "4.673004150390625e-05\n",
      "4.887580871582031e-05\n",
      "5.9604644775390625e-05\n",
      "5.2928924560546875e-05\n",
      "6.437301635742188e-05\n",
      "5.1021575927734375e-05\n",
      "5.245208740234375e-05\n",
      "5.245208740234375e-05\n",
      "5.5789947509765625e-05\n",
      "6.699562072753906e-05\n",
      "4.696846008300781e-05\n",
      "4.6253204345703125e-05\n",
      "4.363059997558594e-05\n",
      "4.267692565917969e-05\n",
      "3.910064697265625e-05\n",
      "4.029273986816406e-05\n",
      "4.00543212890625e-05\n",
      "4.124641418457031e-05\n",
      "4.0531158447265625e-05\n",
      "6.818771362304688e-05\n",
      "4.458427429199219e-05\n",
      "4.410743713378906e-05\n",
      "4.410743713378906e-05\n",
      "4.267692565917969e-05\n",
      "4.2438507080078125e-05\n",
      "4.100799560546875e-05\n",
      "4.0531158447265625e-05\n",
      "3.933906555175781e-05\n",
      "6.4849853515625e-05\n",
      "4.2438507080078125e-05\n",
      "4.553794860839844e-05\n",
      "4.410743713378906e-05\n",
      "4.57763671875e-05\n",
      "4.029273986816406e-05\n",
      "5.412101745605469e-05\n",
      "4.363059997558594e-05\n",
      "4.887580871582031e-05\n",
      "5.412101745605469e-05\n",
      "6.079673767089844e-05\n",
      "5.245208740234375e-05\n",
      "4.5299530029296875e-05\n",
      "4.744529724121094e-05\n",
      "4.8160552978515625e-05\n",
      "4.076957702636719e-05\n",
      "4.601478576660156e-05\n",
      "3.0994415283203125e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t0 = time.time()\n",
    "    outputNN = apply_model(dataNN)\n",
    "    print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jacfwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Define a function that takes only the inputs you want the Jacobian with respect to\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m jac_fn \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjit(jacfwd(\u001b[39mlambda\u001b[39;00m x: predict_outputs(position_now, x,)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_output_wrt_input_data\u001b[39m(input_data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Concatenate position_now and input_data to form the full input\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     dataNN \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate((position_now, input_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jacfwd' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function that takes only the inputs you want the Jacobian with respect to\n",
    "jac_fn = jax.jit(jacfwd(lambda x: predict_outputs(position_now, x,)))\n",
    "\n",
    "@jax.jit\n",
    "def model_output_wrt_input_data(input_data):\n",
    "    # Concatenate position_now and input_data to form the full input\n",
    "    dataNN = jnp.concatenate((position_now, input_data))\n",
    "    return apply_model(dataNN)\n",
    "\n",
    "\n",
    "# Compute the Jacobian of the output with respect to input_data\n",
    "jacobian_fn = jax.jit(jax.jacrev(model_output_wrt_input_data))\n",
    "time_log = []\n",
    "for i in range(100):\n",
    "    t0 = time.time()\n",
    "    jacobian = jacobian_fn(input_data)\n",
    "    time_log.append(time.time() - t0)\n",
    "\n",
    "\n",
    "print(\"Jacobian:\\n\", jacobian)\n",
    "print(\"Computation Time:\", np.array(time_log).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\" Predicts the system output state using a feedforward neural network. \"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m position_now \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mstate_vector\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m curr_thrust \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlast_input[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/factslabegmc/final_wardi_files/src/quad_newton_raphson/quad_newton_raphson/jax_NNs_final.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m curr_rolldot \u001b[39m=\u001b[39m last_input[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        \"\"\" Predicts the system output state using a feedforward neural network. \"\"\"\n",
    "        position_now = self.state_vector.T.tolist()[0]\n",
    "        curr_thrust = -last_input[0][0]\n",
    "        curr_rolldot = last_input[1][0]\n",
    "        curr_pitchdot = last_input[2][0]\n",
    "        curr_yawdot = last_input[3][0]\n",
    "        input_data = [curr_thrust, curr_rolldot, curr_pitchdot, curr_yawdot]\n",
    "        # print(f\"position_now: {position_now}\")\n",
    "        # print(f\"input_data: {input_data}\")\n",
    "\n",
    "        # Concatenate state vector and input vector\n",
    "        state_vector = torch.tensor(position_now, dtype=torch.float32)\n",
    "        input_vector = torch.tensor(input_data, dtype=torch.float32, requires_grad=True)\n",
    "        dataNN = torch.cat([state_vector, input_vector])\n",
    "        # print(f\"dataNN: {dataNN}\")\n",
    "\n",
    "        # print(\"Feed Forward NN\")\n",
    "        # t1 = time.time()\n",
    "        outputNN = self.NN(dataNN)\n",
    "        # print(f\"outputNN: {outputNN}\")\n",
    "\n",
    "        # Compute Jacobian\n",
    "        jacobian = torch.zeros((4, 4))\n",
    "        # print(input_vector.grad)\n",
    "        # input_vector.grad.zero_()\n",
    "\n",
    "        for i in range(4):\n",
    "            self.NN.zero_grad()  # Reset gradients to zero\n",
    "            if outputNN.grad is not None:\n",
    "                outputNN.grad.zero_()\n",
    "            outputNN[i].backward(retain_graph=True)  # Compute gradients\n",
    "            jacobian[i] = input_vector.grad\n",
    "\n",
    "        # print(\"Jacobian Matrix:\\n\", jacobian)\n",
    "\n",
    "        inv_jac = np.linalg.inv(jacobian)\n",
    "        # print(f\"inv_jac: {inv_jac}\")\n",
    "        inv_jac[:, 2] = -inv_jac[:, 2]\n",
    "        # print(f\"inv_jac: {inv_jac}\")\n",
    "\n",
    "        self.jac_inv = inv_jac\n",
    "\n",
    "        outputNN = outputNN.detach().numpy()\n",
    "        outputNN = np.array([[outputNN[0], outputNN[1], outputNN[2], outputNN[3]]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
